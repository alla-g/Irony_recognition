{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating feature dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import jamspell\n",
    "import nltk\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from simple_elmo import ElmoModel\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from collections import Counter\n",
    "\n",
    "from math import log\n",
    "from statistics import mean\n",
    "from random import choices\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "m = MorphAnalyzer()\n",
    "\n",
    "corrector = jamspell.TSpellCorrector()\n",
    "corrector.LoadLangModel('ru_small.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences in total: 8956\n",
      "Ironic sentences: 1001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Ironic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>У человека-летучей мыши обнаружен Covid-19</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Съемки нового фильма «Бэтмен» приостановлены из-за того, что исполнитель главной роли Роберт Паттинсон заразился коронавирусом, передает Liter.kz со ссылкой на издание Vanity Fair.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>В официальном заявлении киностудии Warner Bros. не уточняется, у кого из участников съемочной группы  выявили положительный результат на COVID-19.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Здесь лишь сообщили, что инфицированный находится в изоляции, а съемки на время приостановлены.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>По данным источника издания Vanity Fair, коронавирусом заразился Роберт Паттинсон, исполняющий главную роль в фильме.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                               Sentence  \\\n",
       "0  У человека-летучей мыши обнаружен Covid-19                                                                                                                                             \n",
       "1  Съемки нового фильма «Бэтмен» приостановлены из-за того, что исполнитель главной роли Роберт Паттинсон заразился коронавирусом, передает Liter.kz со ссылкой на издание Vanity Fair.   \n",
       "2  В официальном заявлении киностудии Warner Bros. не уточняется, у кого из участников съемочной группы  выявили положительный результат на COVID-19.                                     \n",
       "3  Здесь лишь сообщили, что инфицированный находится в изоляции, а съемки на время приостановлены.                                                                                        \n",
       "4  По данным источника издания Vanity Fair, коронавирусом заразился Роберт Паттинсон, исполняющий главную роль в фильме.                                                                  \n",
       "\n",
       "   Ironic  \n",
       "0  1.0     \n",
       "1  0.0     \n",
       "2  0.0     \n",
       "3  0.0     \n",
       "4  0.0     "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train_corpus.csv', sep=';')\n",
    "df.dropna(inplace=True)\n",
    "df = df.drop(['Post tag'], axis=1)\n",
    "print(f'Sentences in total: {len(df)}')\n",
    "print(f'Ironic sentences: {df[\"Ironic\"].value_counts()[1.0]}')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Irony markers extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weirdness(target: str, contrast: str):\n",
    "    target_words = [word for sent in target for word in word_tokenize(sent) if word.isalpha()]\n",
    "    len_target = len(target_words)\n",
    "    target_lemmas = [m.parse(word)[0].normal_form for word in target_words]\n",
    "    target_freqs = Counter(target_lemmas)\n",
    "\n",
    "    contrast_words = [word for sent in contrast for word in word_tokenize(sent) if word.isalpha()]\n",
    "    len_contrast = len(contrast_words)\n",
    "    contrast_lemmas = [m.parse(word)[0].normal_form for word in contrast_words]\n",
    "    contrast_freqs = Counter(contrast_lemmas)\n",
    "\n",
    "    coefs = {}\n",
    "    for word in target_freqs:\n",
    "        coefs[word] = (target_freqs[word] / len_target) / ((contrast_freqs[word]+target_freqs[word]) / (len_contrast+len_target))\n",
    "\n",
    "    return coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglikelihood(target: str, contrast: str):\n",
    "    target_words = [word for sent in target for word in word_tokenize(sent) if word.isalpha()]\n",
    "    len_target = len(target_words)\n",
    "    target_lemmas = [m.parse(word)[0].normal_form for word in target_words]\n",
    "    target_freqs = Counter(target_lemmas)\n",
    "\n",
    "    contrast_words = [word for sent in contrast for word in word_tokenize(sent) if word.isalpha()]\n",
    "    len_contrast = len(contrast_words)\n",
    "    contrast_lemmas = [m.parse(word)[0].normal_form for word in contrast_words]\n",
    "    contrast_freqs = Counter(contrast_lemmas)\n",
    "   \n",
    "    coefs = {}\n",
    "    for word in target_freqs:\n",
    "        d_targ = len_target * (target_freqs[word] + contrast_freqs[word]) / (len_target + len_contrast)\n",
    "        d_contr = len_contrast * (target_freqs[word] + contrast_freqs[word]) / (len_target + len_contrast)\n",
    "        coefs[word] = 2*((target_freqs[word]*log(target_freqs[word]/d_targ)) + (contrast_freqs[word]*log(contrast_freqs[word]/d_contr)))\n",
    "    \n",
    "    return coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df[df['Ironic'] == 1]['Sentence'].values.tolist()\n",
    "contrast = df['Sentence'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('копчик', 3.9704838336291384),\n",
       " ('накалиться', 3.9704838336291384),\n",
       " ('ежели', 3.9704838336291384),\n",
       " ('пытливый', 3.9704838336291384),\n",
       " ('знаток', 3.9704838336291384),\n",
       " ('позорный', 3.9704838336291384),\n",
       " ('пригвоздить', 3.9704838336291384),\n",
       " ('альтернативноодаренный', 3.9704838336291384),\n",
       " ('школоть', 3.9704838336291384),\n",
       " ('ничуть', 3.9704838336291384)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_weird = weirdness(target, contrast)\n",
    "Counter(top_weird).most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('https', 26.127949595009547),\n",
       " ('человек', 17.14162764118675),\n",
       " ('страна', 15.570015806263505),\n",
       " ('демократический', 13.701574411968817),\n",
       " ('ты', 13.170662824330812),\n",
       " ('чистый', 11.934076150707632),\n",
       " ('случайность', 11.113824330289322),\n",
       " ('режим', 10.916293929684276),\n",
       " ('яков', 10.249708432234861),\n",
       " ('свобода', 9.669731794700796)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_logl = loglikelihood(target, contrast)\n",
    "Counter(top_logl).most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ironies = df.loc[df['Ironic'] == 1]\n",
    "non_ironies = df.loc[df['Ironic'] == 0]\n",
    "non_ironies = non_ironies.sample(1001)\n",
    "norm_df = pd.concat([ironies, non_ironies], sort=False, axis=0)\n",
    "norm_df = norm_df.sample(frac=1).reset_index(drop=True)\n",
    "norm_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weird_markers = list(map(lambda x: x[0], top_weird))[:10]\n",
    "', '.join(weird_markers)\n",
    "\n",
    "logl_markers = list(map(lambda x: x[0], top_logl))\n",
    "', '.join(logl_markers)\n",
    "\n",
    "interjs = ['бы', 'ах', 'эх', 'ой', 'ох', 'оу', 'угу', 'ага', 'вау', 'ха', 'ха-ха', 'ух']\n",
    "\n",
    "pat = re.compile('[«|\\\"](.+)[»|\\\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quotes(sent):\n",
    "    inbrs = re.findall(pat, sent)\n",
    "    for inbr in inbrs:\n",
    "        if inbr[0].islower() and ' ' not in inbr:\n",
    "            return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclam(sent):\n",
    "    return sent.count('!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quest(sent):\n",
    "    return sent.count('?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mult_br(sent):\n",
    "    if '))' in sent or '((' in sent:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intj(sent):\n",
    "    sent_words = word_tokenize(sent)\n",
    "    for word in interjs:\n",
    "        if word in sent_words or word.upper() in sent_words:\n",
    "            return 1 \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weird_mrkers(sent):\n",
    "    lemmas = [m.parse(word)[0].normal_form for word in word_tokenize(sent) if word.isalpha()]\n",
    "    for word in weird_markers:\n",
    "        if word in lemmas:\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def if_mistake(sent):\n",
    "    sent2 = corrector.FixFragment(sent)\n",
    "    if sent == sent2:\n",
    "        return 0\n",
    "    else:\n",
    "        count = 0\n",
    "        for w1, w2 in zip(word_tokenize(sent), word_tokenize(sent2)):\n",
    "            if w1 != w2:\n",
    "                count += 1\n",
    "        return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_vocab = pd.read_csv('concat_vocab.csv', encoding='UTF-8', sep=';')\n",
    "wordlist = concat_vocab['term'].tolist()\n",
    "print(len(wordlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrast_tone(sent, flag):\n",
    "    tag_list = []\n",
    "    for word in word_tokenize(sent):\n",
    "        if word.isalpha():\n",
    "            lemma = m.parse(word)[0].normal_form\n",
    "            if lemma in wordlist:\n",
    "                tag = list(concat_vocab.loc[concat_vocab['term'] == lemma]['tag'])[0]\n",
    "            else:\n",
    "                tag = '-'\n",
    "            tag_list.append(tag)\n",
    "            \n",
    "    if flag == 'pos_and_neg':\n",
    "        if 'PSTV' in tag_list and 'NGTV' in tag_list:\n",
    "            return 1\n",
    "        return 0\n",
    "    \n",
    "    if flag == 'pos_near_neg':\n",
    "        for n in range(len(tag_list)-1):\n",
    "            if tag_list[n] == 'PSTV' and tag_list[n+1] == 'NGTV':\n",
    "                return 1\n",
    "            if tag_list[n] == 'NGTV' and tag_list[n+1] == 'PSTV':\n",
    "                return 1\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sg = KeyedVectors.load_word2vec_format('model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipgram_vec(sent):\n",
    "    tokens = []\n",
    "    for word in word_tokenize(sent):\n",
    "        if word.isalpha():\n",
    "            result = m.parse(word)[0]\n",
    "            lemma = result.normal_form\n",
    "            pos = result.tag.POS\n",
    "            if pos == 'NOUN' or pos == 'VERB':\n",
    "                if f'{lemma}_{pos}' in model_sg.vocab:\n",
    "                    tokens.append(f'{lemma}_{pos}')\n",
    "    if tokens == []:\n",
    "        return 0\n",
    "    vecs = [model_sg[t] for t in tokens]\n",
    "    center = np.mean(vecs, axis=0)\n",
    "\n",
    "    max_d = max(model_sg.distances(center, tokens))\n",
    "    return abs(max_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_elmo = ElmoModel()\n",
    "model_elmo.load('199.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elmo_vec(sent):\n",
    "    tokens = []\n",
    "    for word in word_tokenize(sent):\n",
    "        if word.isalpha():\n",
    "            result = m.parse(word)[0]\n",
    "            lemma = result.normal_form\n",
    "            pos = result.tag.POS\n",
    "            if pos == 'NOUN' or pos == 'VERB' or pos == 'ADJF':\n",
    "                tokens.append(lemma)\n",
    "    if tokens == []:\n",
    "        return 0\n",
    "    vecs = model_elmo.get_elmo_vector_average(tokens)\n",
    "    center = np.mean(vecs, axis=0)\n",
    "\n",
    "    min_sim = min(model_sg.cosine_similarities(center, vecs))\n",
    "    idx = list(model_sg.cosine_similarities(center, vecs)).index(min_sim)\n",
    "    return 1 - min_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main corpus (2 002 sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_df['Quotes'] = norm_df['Sentence'].apply(quotes)\n",
    "norm_df['Exclamation'] = norm_df['Sentence'].apply(exclam)\n",
    "norm_df['Question'] = norm_df['Sentence'].apply(quest)\n",
    "norm_df['Multiple brackets'] = norm_df['Sentence'].apply(mult_br)\n",
    "norm_df['Interjections'] = norm_df['Sentence'].apply(intj)\n",
    "norm_df['Weird markers'] = norm_df['Sentence'].apply(weird_mrkers)\n",
    "norm_df['Mistakes'] = norm_df['Sentence'].apply(if_mistake)\n",
    "norm_df['P&N'] = norm_df['Sentence'].apply(contrast_tone, flag ='pos_and_neg')\n",
    "norm_df['P near N'] = norm_df['Sentence'].apply(contrast_tone, flag ='pos_near_neg')\n",
    "norm_df['Max vec dist'] = norm_df['Sentence'].apply(elmo_vec)\n",
    "norm_df['Max vec sg dist'] = norm_df['Sentence'].apply(skipgram_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_df.to_csv('train_features.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test corpus (100 sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences in total: 100\n",
      "Ironic sentences: 30\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Ironic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Можете спать спокойно — террористы не пройдут!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Во всём мире люди очень боятся терактов, даже произнесение слова \"террорист\" у многих вызывает панику.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Но у меня для вас прекрасные новости из Кургана!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Там наконец пришли к решению этой глобальной проблемы.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Теперь можно спать спокойно.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                 Sentence  \\\n",
       "0  Можете спать спокойно — террористы не пройдут!                                                           \n",
       "1  Во всём мире люди очень боятся терактов, даже произнесение слова \"террорист\" у многих вызывает панику.   \n",
       "2  Но у меня для вас прекрасные новости из Кургана!                                                         \n",
       "3  Там наконец пришли к решению этой глобальной проблемы.                                                   \n",
       "4  Теперь можно спать спокойно.                                                                             \n",
       "\n",
       "   Ironic  \n",
       "0  1       \n",
       "1  0       \n",
       "2  1       \n",
       "3  1       \n",
       "4  1       "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('test_corpus.csv', sep=';')\n",
    "print(f'Sentences in total: {len(test_df)}')\n",
    "print(f'Ironic sentences: {test_df[\"Ironic\"].value_counts()[1.0]}')\n",
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Quotes'] = test_df['Sentence'].apply(quotes)\n",
    "test_df['Exclamation'] = test_df['Sentence'].apply(exclam)\n",
    "test_df['Question'] = test_df['Sentence'].apply(quest)\n",
    "test_df['Multiple brackets'] = test_df['Sentence'].apply(mult_br)\n",
    "test_df['Interjections'] = test_df['Sentence'].apply(intj)\n",
    "test_df['Weird markers'] = test_df['Sentence'].apply(weird_mrkers)\n",
    "test_df['Mistakes'] = test_df['Sentence'].apply(if_mistake)\n",
    "test_df['P&N'] = test_df['Sentence'].apply(contrast_tone, flag ='pos_and_neg')\n",
    "test_df['P near N'] = test_df['Sentence'].apply(contrast_tone, flag ='pos_near_neg')\n",
    "test_df['Max vec dist'] = test_df['Sentence'].apply(elmo_vec)\n",
    "test_df['Max vec sg dist'] = test_df['Sentence'].apply(skipgram_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('test_features.csv', index=False, sep=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
